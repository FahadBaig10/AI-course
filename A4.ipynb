{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Classify language out of the list given below using just stop words. Remove punctuations, make lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19i1883\n",
    "#Fahad Baig\n",
    "#section: J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output {'arabic': 0, 'azerbaijani': 1, 'danish': 0, 'dutch': 3, 'english': 5, 'finnish': 0, 'french': 1, 'german': 1, 'greek': 0, 'hungarian': 1, 'indonesian': 1, 'italian': 2, 'kazakh': 0, 'nepali': 0, 'norwegian': 0, 'portuguese': 1, 'romanian': 1, 'russian': 0, 'slovene': 0, 'spanish': 1, 'swedish': 0, 'tajik': 0, 'turkish': 0}\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.text import Text\n",
    "\n",
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\"\n",
    "T=Test.lower()\n",
    "#T=word_tokenize(T)\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "T = tokenizer.tokenize(T)\n",
    "\n",
    "#display((Text(T)))\n",
    "T=set(T)\n",
    "\n",
    "\n",
    "################################\n",
    "ara=0\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        ara+=1\n",
    "#print(ara)\n",
    "aze=0\n",
    "stop_words = set(stopwords.words('azerbaijani'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        aze+=1\n",
    "#print(aze)\n",
    "\n",
    "dan=0\n",
    "stop_words = set(stopwords.words('danish'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        dan+=1\n",
    "#print(dan)\n",
    "\n",
    "dut=0\n",
    "stop_words = set(stopwords.words('dutch'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        dut+=1\n",
    "#print(dut)\n",
    "\n",
    "\n",
    "#################\n",
    "eng=0\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        eng+=1\n",
    "#print(eng)\n",
    "###################\n",
    "fin=0\n",
    "stop_words = set(stopwords.words('finnish'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        fin+=1\n",
    "#print(fin)\n",
    "\n",
    "fre=0\n",
    "stop_words = set(stopwords.words('french'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        fre+=1\n",
    "#print(fre)\n",
    "\n",
    "ger=0\n",
    "stop_words = set(stopwords.words('german'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        ger+=1\n",
    "#print(ger)\n",
    "\n",
    "gre=0\n",
    "stop_words = set(stopwords.words('greek'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        gre+=1\n",
    "#print(gre)\n",
    "\n",
    "hun=0\n",
    "stop_words = set(stopwords.words('hungarian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        hun+=1\n",
    "#print(hun)\n",
    "\n",
    "ind=0\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        ind+=1\n",
    "#print(ind)\n",
    "\n",
    "ita=0\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        ita+=1\n",
    "#print(ita)\n",
    "\n",
    "kaz=0\n",
    "stop_words = set(stopwords.words('kazakh'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        kaz+=1\n",
    "#print(kaz)\n",
    "\n",
    "\n",
    "nep=0\n",
    "stop_words = set(stopwords.words('nepali'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        nep+=1\n",
    "#print(nep)\n",
    "\n",
    "nor=0\n",
    "stop_words = set(stopwords.words('norwegian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        nor+=1\n",
    "#print(nor)\n",
    "\n",
    "por=0\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        por+=1\n",
    "#print(por)\n",
    "\n",
    "rom=0\n",
    "stop_words = set(stopwords.words('romanian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        rom+=1\n",
    "#print(rom)\n",
    "\n",
    "rom=0\n",
    "stop_words = set(stopwords.words('romanian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        rom+=1\n",
    "#print(rom)\n",
    "\n",
    "rus=0\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        rus+=1\n",
    "#print(rus)\n",
    "\n",
    "slo=0\n",
    "stop_words = set(stopwords.words('slovene'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        slo+=1\n",
    "#print(slo)\n",
    "\n",
    "spa=0\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        spa+=1\n",
    "#print(spa)\n",
    "\n",
    "swe=0\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        swe+=1\n",
    "#print(swe)\n",
    "\n",
    "taj=0\n",
    "stop_words = set(stopwords.words('tajik'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        taj+=1\n",
    "#print(taj)\n",
    "\n",
    "tur=0\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "filtered_sentence = []\n",
    "for w in T:\n",
    "     if w in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        tur+=1\n",
    "#print(tur)\n",
    "Dict = {'arabic':ara ,'azerbaijani':aze ,'danish':dan ,'dutch':dut ,'english':eng ,'finnish':fin ,'french':fre ,'german':ger ,'greek':gre ,'hungarian':hun ,'indonesian':ind ,'italian':ita ,'kazakh':kaz ,'nepali':nep ,'norwegian':nor ,'portuguese':por ,'romanian':rom ,'russian':rus ,'slovene':slo ,'spanish':spa ,'swedish':swe ,'tajik':taj ,'turkish':tur}\n",
    "print('output',Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "{'arabic': 0,\n",
    " 'azerbaijani': 1,\n",
    " 'danish': 0,\n",
    " 'dutch': 3,\n",
    " 'english': 5,\n",
    " 'finnish': 0,\n",
    " 'french': 1,\n",
    " 'german': 1,\n",
    " 'greek': 0,\n",
    " 'hungarian': 1,\n",
    " 'indonesian': 1,\n",
    " 'italian': 2,\n",
    " 'kazakh': 0,\n",
    " 'nepali': 0,\n",
    " 'norwegian': 0,\n",
    " 'portuguese': 1,\n",
    " 'romanian': 1,\n",
    " 'romanurdu': 1,\n",
    " 'russian': 0,\n",
    " 'slovene': 0,\n",
    " 'spanish': 1,\n",
    " 'swedish': 0,\n",
    " 'tajik': 0,\n",
    " 'turkish': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: make a Roman urdu sentence tokenizer by assuming that there will be no (.full stop) and (? question mark) in the end of sentence. Make some rules to made that tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output \"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain\", \"ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi\", \"is aylominim ion battery ko jet laboratory , larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai jisay aik karne ke baad taqreeban 10 Mahtaq dobarah charge karne ki zaroorat nahi rehti hai\", \"tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha\", \"team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\"\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "Test2=\"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi is aylominim ion battery ko jet laboratory, larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai jisay aik karne ke baad taqreeban 10 Mahtaq dobarah charge karne ki zaroorat nahi rehti hai tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert  grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\"\n",
    "\n",
    "\n",
    "string = '\\\"'\n",
    "romanurdu_n_w =  ['kijiye','gay', 'honge', 'ga','hai','gai','gaya','h',\n",
    "                       'chahiye', 'hoyein', 'gi', 'kijaye', 'gayin', 'thin', 'hun', 'hon', 'khareeda',\n",
    "                      'tha', 'thi', 'thay', 'hain', 'ha']\n",
    "\n",
    "romanurdu=[\n",
    "        \"ai\", \"ayi\", \"ha\",\"hy\", \"hai\",\"hain\", \"main\", \"ki\", \"tha\", \"koi\", \"ko\", \"sy\", \"woh\", \n",
    "        \"bhi\", \"aur\", \"wo\", \"yeh\", \"rha\", \"hota\", \"ho\", \"ga\", \"ka\", \"le\", \"lye\", \n",
    "        \"kr\", \"kar\", \"lye\", \"liye\", \"hotay\", \"waisay\", \"gya\", \"gi\",\"gaya\", \"kch\", \"ab\",\n",
    "        \"thy\", \"thay\", \"houn\", \"hain\", \"han\", \"to\", \"is\", \"hi\", \"jo\", \"kya\", \"thi\",\n",
    "        \"se\", \"pe\", \"phr\", \"wala\", \"waisay\", \"us\", \"na\", \"ny\", \"hun\", \"rha\", \"raha\",\n",
    "        \"ja\", \"rahay\", \"abi\", \"uski\", \"ne\", \"haan\", \"acha\", \"nai\", \"sent\", \"photo\", \n",
    "        \"you\", \"kafi\", \"gai\", \"rhy\", \"kuch\", \"jata\", \"aye\", \"ya\", \"dono\", \"hoa\", \n",
    "        \"aese\", \"de\", \"wohi\", \"jati\", \"jb\", \"krta\", \"lg\", \"rahi\", \"hui\", \"karna\", \n",
    "        \"krna\", \"gi\", \"hova\", \"yehi\", \"jana\", \"jye\", \"chal\", \"mil\", \"tu\", \"hum\", \"par\", \n",
    "        \"hay\", \"kis\", \"sb\", \"gy\", \"dain\", \"krny\", \"tou\"\n",
    "    ]\n",
    "\n",
    "lis = ['ka','ki','ko','nay','kay']\n",
    "\n",
    "romanurdu_connect = ['jinhein', 'jis', 'jin', 'jo',\n",
    "                    'aur', 'agar', 'agarcha', 'lekin', 'magar',\n",
    "                    'par', 'ya', 'tahim', 'ke', 'kar','to',\n",
    "                    'gay', 'gi','jisney','jisay','jisey']\n",
    "Test2 = word_tokenize(Test2)\n",
    "\n",
    "for a in range(len(Test2)):\n",
    "    if Test2[a] in romanurdu_n_w:\n",
    "        if a == len(Test2)-1:\n",
    "            string = string + Test2[a]\n",
    "            string = string + '\\\"'\n",
    "            \n",
    "        elif a < len(Test2)-1:\n",
    "           \n",
    "            if Test2[a+2] in lis:\n",
    "                string = string + Test2[a]\n",
    "                string = string + ' '\n",
    "                         # #Test2 = tokenizer.tokenize(Test2)\n",
    "# Test2 = Test2.split()\n",
    "# new=[]\n",
    "\n",
    "# for k,i in enumerate(Test2):\n",
    "#      k=Test2\n",
    "#     for j in enumerate(romanurdu):\n",
    "#          if i==j:\n",
    "#             new=new+''\n",
    "#             print(new)\n",
    "# print(str(new))       \n",
    "\n",
    "\n",
    "#             # Test2.append(1)\n",
    "#             #  test2=test2+''\n",
    "# #print(Test2)\n",
    "# #print(splits) \n",
    "            elif Test2[a+1] in romanurdu_connect:\n",
    "                string = string + Test2[a]\n",
    "                string = string + ' '\n",
    "\n",
    "            else:\n",
    "                string = string + Test2[a]\n",
    "                string = string + '\\\", \\\"'\n",
    "\n",
    "    else:\n",
    "        string = string + Test2[a]\n",
    "        string = string + ' '\n",
    "    \n",
    "print('Output',string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test2=\"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi is aylominim ion battery ko jet laboratory, larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai jisay aik karne ke baad taqreeban 10 Mahtaq dobarah charge karne ki zaroorat nahi rehti hai tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert  grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain\", \"ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi\", \"is aylominim ion battery ko jet laboratory, larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai\", \"jisay aik karne ke baad taqreeban 10 Mah taq dobarah charge karne ki zaroorat nahi rehti hai\", \"tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert  grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha\", \"team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
